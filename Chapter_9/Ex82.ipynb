{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:105: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, symbols)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:661: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, \"\")\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:173: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(rf\"([{string.punctuation}])+\", symbol)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:105: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, symbols)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:661: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, \"\")\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:173: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(rf\"([{string.punctuation}])+\", symbol)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3124\\1017142510.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_title'] = self.preprocess(df)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:105: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, symbols)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:661: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, \"\")\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:173: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(rf\"([{string.punctuation}])+\", symbol)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 132, 1548,    0,  ..., 5332,  395,  188],\n",
      "        [5400, 1413, 2670,  ...,   22,    5,  860],\n",
      "        [   9,    0, 1085,  ...,  166,   98, 2034],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]], dtype=torch.int32)\n",
      "tensor([ 132, 1548,    0,  ..., 5332,  395,  188], dtype=torch.int32)\n",
      "tensor([[ 814,   25,    0,  ...,  113,  279,    0],\n",
      "        [ 815,   28, 1832,  ...,    1, 1491, 2151],\n",
      "        [ 973,   25,    0,  ..., 6128,  908, 1157],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]], dtype=torch.int32)\n",
      "tensor([814,  25,   0,  ..., 113, 279,   0], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3124\\1017142510.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_title'] = self.preprocess(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([303, 10672, 4])\n",
      "epoch: 1, tr_loss: 0.5336, tr_acc: 0.3961, val_loss: 0.5339, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 2, tr_loss: 0.5332, tr_acc: 0.3961, val_loss: 0.5331, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 3, tr_loss: 0.5323, tr_acc: 0.3961, val_loss: 0.5320, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 4, tr_loss: 0.5311, tr_acc: 0.3961, val_loss: 0.5306, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 5, tr_loss: 0.5296, tr_acc: 0.3961, val_loss: 0.5290, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 6, tr_loss: 0.5279, tr_acc: 0.3961, val_loss: 0.5272, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 7, tr_loss: 0.5259, tr_acc: 0.3961, val_loss: 0.5253, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 8, tr_loss: 0.5237, tr_acc: 0.3961, val_loss: 0.5232, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 9, tr_loss: 0.5215, tr_acc: 0.3961, val_loss: 0.5211, val_acc: 0.4018\n",
      "torch.Size([303, 10672, 4])\n",
      "epoch: 10, tr_loss: 0.5192, tr_acc: 0.3961, val_loss: 0.5190, val_acc: 0.4018\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import texthero as hero\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def calc_acc(tensor_pred, tensor_label: torch.tensor) -> float:\n",
    "    y_te_pred = torch.argmax(tensor_pred, dim=1)\n",
    "    y_label = torch.argmax(tensor_label, dim=1)\n",
    "    acc = (y_te_pred == y_label).sum().item() / y_label.shape[0]\n",
    "\n",
    "    assert acc >= 0 and acc <= 1\n",
    "    return acc\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def words_to_ids(df: pd.DataFrame):\n",
    "    dictionary = {}\n",
    "\n",
    "    df[\"clean-title\"] = hero.clean(df['title'])\n",
    "    titles = df['clean-title'].tolist()\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word in dictionary:\n",
    "                dictionary[word] += 1\n",
    "            else:\n",
    "                dictionary[word] = 1\n",
    "    idx = 1\n",
    "    # print(Counter(dictionary))\n",
    "    # print((sorted(dictionary, key=dictionary.get, reverse=True)))\n",
    "    for word in sorted(dictionary, key=dictionary.get, reverse=True):\n",
    "        if dictionary[word] == 1:\n",
    "            dictionary[word] = 0\n",
    "        else:\n",
    "            dictionary[word] = idx\n",
    "            idx += 1\n",
    "    sorted_dict = dict(sorted(dictionary.items(), key=lambda x: x[1]))\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, filepath):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return hero.clean(df)\n",
    "\n",
    "    def make_feature(self, titles: list, dic: dict) -> torch.tensor:\n",
    "        X = []\n",
    "        maxLen = 0\n",
    "        for i, title in enumerate(titles):\n",
    "            words = title.split(' ')\n",
    "            if (len(words) >= maxLen):\n",
    "                maxLen = len(words)\n",
    "            l = []\n",
    "            for word in words:\n",
    "                if word in dic:\n",
    "                    l.append(dic[word])\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            # n_samples x seq_len x  (10672 x variable_titlelen: max 303)\n",
    "            X.append(torch.tensor(l, dtype=torch.int))\n",
    "        # 303 x 10672, first row correspond to every first words of the articles\n",
    "        X = nn.utils.rnn.pad_sequence(X)\n",
    "        print(X)\n",
    "        print(X[0])\n",
    "        return X\n",
    "\n",
    "    def make_feature_pipeline(self, df: pd.DataFrame,\n",
    "                              dic: dict) -> torch.tensor:\n",
    "        df['clean_title'] = self.preprocess(df)\n",
    "        titles = df['clean_title'].tolist()\n",
    "        return self.make_feature(titles=titles, dic=dic)\n",
    "\n",
    "\n",
    "class RNN(nn.Module): \n",
    "    def __init__(self, input_size: int,  # 300\n",
    "                 hidden_size: int,  # 50\n",
    "                 output_size: int,  # 4\n",
    "                 n_vocab: int):  # 12455\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            n_vocab, input_size)  # n_vocab = vocab_size\n",
    "        self.rnn = nn.RNN(input_size=input_size,  # 300\n",
    "                          hidden_size=hidden_size,  # 50\n",
    "                          num_layers= 1,\n",
    "                          nonlinearity='tanh',  # activation function\n",
    "                          bias=True,\n",
    "                          bidirectional=False)\n",
    "        self.fc = nn.Linear(in_features=hidden_size,  # 50\n",
    "                            out_features=output_size,  # 4\n",
    "                            bias=True)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    def forward(self, x: torch.tensor, h_0: torch.tensor):\n",
    "        x = self.embedding(x)  # seq_len x n_samples x n_dim(embed) (303,10672,300)\n",
    "        # x = x.permute(1, 0, 2)\n",
    "        x, h_T = self.rnn(x, h_0) #x dim = (303,10672,50)\n",
    "        x = self.fc(x) #(303,10672,4)\n",
    "        x = self.softmax(x)\n",
    "        return x, h_T    \n",
    "    \n",
    "    \n",
    "def train(config: dict):\n",
    "    seed_everything()\n",
    "    filedir_in_6 = '../Data/Output/Chapter6/'\n",
    "    filedir_in_8 = '../Data/Output/Chapter8/'\n",
    "    filepath_bin = '../Data/Chapter7/GoogleNews-vectors-negative300.bin'\n",
    "    train_path = os.path.join(filedir_in_6, 'ex50-train.txt')\n",
    "    val_path = os.path.join(filedir_in_6, 'ex50-val.txt')\n",
    "\n",
    "    train_label_path = os.path.join(filedir_in_8, 'ex70-train_label.pt')\n",
    "    y_tr_label = torch.load(train_label_path)\n",
    "    y_tr_label = torch.nn.functional.one_hot(y_tr_label.long()).to(torch.float)\n",
    "\n",
    "    val_label_path = os.path.join(filedir_in_8, 'ex70-val_label.pt')\n",
    "    y_val_label = torch.load(val_label_path)\n",
    "    y_val_label = torch.nn.functional.one_hot(y_val_label.long()).to(torch.float)\n",
    "\n",
    "    hidden_size = 50\n",
    "    input_size = 300\n",
    "    output_size = 4\n",
    "\n",
    "    fe = FeatureExtractor(filepath=filepath_bin)\n",
    "    df_train = pd.read_csv(train_path, sep='\\t')\n",
    "    dic = words_to_ids(df_train)\n",
    "    x_train = fe.make_feature_pipeline(df=df_train['title'],\n",
    "                                       dic=dic)\n",
    "    df_val = pd.read_csv(val_path, sep='\\t')\n",
    "    x_val = fe.make_feature_pipeline(df=df_val['title'],\n",
    "                                     dic=dic)\n",
    "\n",
    "    batch_size = x_train.shape[1]\n",
    "    batch_size_val = x_val.shape[1]\n",
    "\n",
    "    net = RNN(input_size=input_size,\n",
    "              hidden_size=hidden_size,\n",
    "              output_size=output_size,\n",
    "              n_vocab=len(dic))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(net.parameters(),\n",
    "                          lr=0.001,\n",
    "                          momentum=0.9)\n",
    "\n",
    "    for epoch in range(config['epoch']):\n",
    "        optimizer.zero_grad()\n",
    "            #reset gradient for each batch\n",
    "        output, h_T = net(x=x_train, h_0=torch.zeros(\n",
    "            1, batch_size, hidden_size))\n",
    "        print(output.shape) #(303,10672,300)\n",
    "        y_pred = output[-1, :, :] #(10672,300)\n",
    "        #prediction for all articles\n",
    "        loss = criterion(y_pred, y_tr_label)\n",
    "        tr_loss = loss.item()\n",
    "        tr_acc = calc_acc(y_pred, y_tr_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #for val data\n",
    "        output, h_T = net(x=x_val, h_0=torch.zeros(\n",
    "            1, batch_size_val, hidden_size))\n",
    "        y_pred = output[-1, :, :]\n",
    "        loss = criterion(y_pred, y_val_label)\n",
    "        val_loss = loss.item()\n",
    "        val_acc = calc_acc(y_pred, y_val_label)\n",
    "\n",
    "        print('epoch: {}, tr_loss: {:.4f}, tr_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}'.format(\n",
    "            epoch+1,\n",
    "            tr_loss,\n",
    "            tr_acc,\n",
    "            val_loss,\n",
    "            val_acc\n",
    "        )\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config = {\n",
    "        'epoch': 10,\n",
    "    }\n",
    "    train(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
