{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:105: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, symbols)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:661: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, \"\")\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:173: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(rf\"([{string.punctuation}])+\", symbol)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:105: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, symbols)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:661: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(pattern, \"\")\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\texthero\\preprocessing.py:173: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  return s.str.replace(rf\"([{string.punctuation}])+\", symbol)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_27576\\3078239424.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['clean_title'] = self.preprocess(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n",
      "12455\n",
      "tensor([[ 132, 1548,    0,  ..., 5332,  395,  188],\n",
      "        [5400, 1413, 2670,  ...,   22,    5,  860],\n",
      "        [   9,    0, 1085,  ...,  166,   98, 2034],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]], dtype=torch.int32)\n",
      "tensor([ 132, 1548,    0,  ..., 5332,  395,  188], dtype=torch.int32)\n",
      "torch.Size([303, 10672, 4])\n",
      "tensor([0.2189, 0.4139, 0.0998, 0.2673], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import texthero as hero\n",
    "import pandas as pd\n",
    "# from map_to_id_80 import IDMapping\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, filepath):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return hero.clean(df)\n",
    "\n",
    "    def make_feature(self, titles: list, dic: dict) -> torch.tensor:\n",
    "        X = []\n",
    "        maxLen = 0\n",
    "        for i, title in enumerate(titles):\n",
    "            words = title.split(' ')\n",
    "            if (len(words) >= maxLen):\n",
    "                maxLen = len(words)\n",
    "            l = []\n",
    "            for word in words:\n",
    "                if word in dic:\n",
    "                    l.append(dic[word])\n",
    "                else:\n",
    "                    l.append(0)\n",
    "            X.append(torch.tensor(l, dtype=torch.int))  # n_samples x seq_len x  (10672 x variable_titlelen: max 303)\n",
    "        print(maxLen)\n",
    "        print(len(dic))\n",
    "        X = nn.utils.rnn.pad_sequence(X) #303 x 10672, first row correspond to every first words of the articles \n",
    "        print(X)\n",
    "        print(X[0])\n",
    "        return X\n",
    "\n",
    "    def make_feature_pipeline(self, df: pd.DataFrame,dic: dict) -> torch.tensor:\n",
    "        df['clean_title'] = self.preprocess(df)\n",
    "        titles = df['clean_title'].tolist()\n",
    "        return self.make_feature(titles=titles, dic=dic)\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size: int,#300\n",
    "                 hidden_size: int, #50\n",
    "                 output_size: int,#4\n",
    "                 n_vocab: int):#12455\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, input_size)#n_vocab = vocab_size\n",
    "        self.rnn = nn.RNN(input_size=input_size,#300\n",
    "                          hidden_size=hidden_size,#50\n",
    "                          num_layers=1,\n",
    "                          nonlinearity='tanh',##activation function\n",
    "                          bias=True,\n",
    "                          bidirectional=False)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, #50\n",
    "                            out_features=output_size, #4\n",
    "                            bias=True)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x: torch.tensor, h_0: torch.tensor):\n",
    "        x = self.embedding(x)  # seq_len x n_samples x n_dim(embed) (303,10672,300)\n",
    "        # x = x.permute(1, 0, 2)\n",
    "        x, h_T = self.rnn(x, h_0) #x dim = (303,10672,50)\n",
    "        x = self.fc(x) #(303,10672,4)\n",
    "        x = self.softmax(x)\n",
    "        return x, h_T\n",
    "\n",
    "\n",
    "def words_to_ids(df: pd.DataFrame):\n",
    "    dictionary = {}\n",
    "\n",
    "    df[\"clean-title\"] = hero.clean(df['title'])\n",
    "    titles = df['clean-title'].tolist()\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word in dictionary:\n",
    "                dictionary[word] += 1\n",
    "            else:\n",
    "                dictionary[word] = 1\n",
    "    idx = 1\n",
    "    # print(Counter(dictionary))\n",
    "    # print((sorted(dictionary, key=dictionary.get, reverse=True)))\n",
    "    for word in sorted(dictionary, key=dictionary.get, reverse=True):\n",
    "        if dictionary[word] == 1:\n",
    "            dictionary[word] = 0\n",
    "        else:\n",
    "            dictionary[word] = idx\n",
    "            idx += 1\n",
    "    sorted_dict = dict(sorted(dictionary.items(), key=lambda x: x[1]))\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "filepath_bin = '../Data/Chapter7/GoogleNews-vectors-negative300.bin'\n",
    "filedir_text = '../Data/Output/Chapter6/'\n",
    "train_path = os.path.join(filedir_text, 'Ex50-train.txt')\n",
    "\n",
    "hidden_size = 50\n",
    "input_size = 300\n",
    "output_size = 4 #The number of categories\n",
    "\n",
    "fe = FeatureExtractor(filepath=filepath_bin)\n",
    "df_train = pd.read_csv(train_path, sep='\\t')\n",
    "\n",
    "dic = words_to_ids(df_train)\n",
    "x_train = fe.make_feature_pipeline(df=df_train['title'],\n",
    "                                    dic=dic) #303 x 10672\n",
    "#each row of size 10672 represents the i-th index word of all the titles,  and 303 is the length of the longest title\n",
    "batch_size = x_train.shape[1] ##10672\n",
    "\n",
    "net = RNN(input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            n_vocab=len(dic))\n",
    "output, h_T = net(x_train, h_0=torch.zeros(1, batch_size, hidden_size))\n",
    "print(output.shape) #(303,10672,4)  each of (10672,4) matrix represents for the i-th character of the articles in all the articles, which goes as a sequence. Output of i-th matrix is fed into the next (i+1)th matrix\n",
    "\n",
    "print(output[-1, 0, :]) #the output as for category distribution for the first article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
