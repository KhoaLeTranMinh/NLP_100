{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5623, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor([[ 2.1097e-06, -1.6584e-06, -4.6904e-07,  ..., -1.1038e-06,\n",
      "          1.3178e-06,  3.3685e-06],\n",
      "        [-7.4744e-07,  5.0572e-07,  1.3389e-07,  ...,  4.3032e-07,\n",
      "         -4.4466e-07, -1.1497e-06],\n",
      "        [-7.0701e-07,  5.9917e-07,  1.7039e-07,  ...,  3.5053e-07,\n",
      "         -4.5378e-07, -1.1540e-06],\n",
      "        [-6.5526e-07,  5.5355e-07,  1.6476e-07,  ...,  3.2295e-07,\n",
      "         -4.1934e-07, -1.0648e-06]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "        ...,\n",
      "        [0.2501, 0.2502, 0.2498, 0.2499],\n",
      "        [0.2524, 0.2479, 0.2507, 0.2491],\n",
      "        [0.2586, 0.2479, 0.2540, 0.2395]], grad_fn=<SoftmaxBackward0>)\n",
      "5.545177444479562\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class Net(nn.Module):  # inheritance in python\n",
    "    def __init__(self, in_shape: int, out_shape: int):\n",
    "        super().__init__()\n",
    "        # in_shape = 300 , ##out_shape = 4 which are numbers of categories. This basically creates a \"weight matrix of out_shape x in_shape\"\n",
    "        self.linearTrans = nn.Linear(in_shape, out_shape, bias=True)\n",
    "\n",
    "        nn.init.constant_(self.linearTrans.bias.data, 0)\n",
    "        # for now we make all bias values to 0\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linearTrans(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "filedir_in = '../Data/Output/Chapter8/'\n",
    "\n",
    "train_path = os.path.join(filedir_in, 'Ex70-train.pt')\n",
    "x_train = torch.load(train_path)\n",
    "\n",
    "net = Net(in_shape=x_train.shape[1], out_shape=4)\n",
    "y_pred = net(x_train)\n",
    "\n",
    "train_label_path = os.path.join(filedir_in, 'ex70-train_label.pt')\n",
    "y_label = torch.load(train_label_path)\n",
    "y_label = torch.nn.functional.one_hot(y_label.long()).to(torch.float)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "output = loss(y_pred, y_label)\n",
    "print(output)\n",
    "output.backward()\n",
    "#backward propagation, which basically optimizes the parameter by using gradient descent\n",
    "print(net.linearTrans.weight.grad)\n",
    "print(y_label)\n",
    "print(y_pred)\n",
    "# assert net.linearTrans.weight.grad.shape == net.linearTrans.weight.shape\n",
    "print(-math.log(0.25)*4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
