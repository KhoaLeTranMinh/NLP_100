{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 153348\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 6750\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 6970\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae33857a4b9497d973b85776dcb2224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/153348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aeb6adba19548bdb6349e54a0b8e893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9653faad3264f7aa3438e435866be36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e67e7fea22542a7afeb953e056f6d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.9699, 'learning_rate': 1.8956703182055298e-05, 'epoch': 0.05}\n",
      "{'loss': 10.9699, 'learning_rate': 1.791340636411059e-05, 'epoch': 0.1}\n",
      "{'loss': 10.9699, 'learning_rate': 1.6870109546165886e-05, 'epoch': 0.16}\n",
      "{'loss': 10.9699, 'learning_rate': 1.582681272822118e-05, 'epoch': 0.21}\n",
      "{'loss': 10.9699, 'learning_rate': 1.4783515910276475e-05, 'epoch': 0.26}\n",
      "{'loss': 10.9699, 'learning_rate': 1.374021909233177e-05, 'epoch': 0.31}\n",
      "{'loss': 10.9699, 'learning_rate': 1.2696922274387064e-05, 'epoch': 0.37}\n",
      "{'loss': 10.9699, 'learning_rate': 1.1653625456442358e-05, 'epoch': 0.42}\n",
      "{'loss': 10.9699, 'learning_rate': 1.0610328638497653e-05, 'epoch': 0.47}\n",
      "{'loss': 10.9699, 'learning_rate': 9.567031820552947e-06, 'epoch': 0.52}\n",
      "{'loss': 10.9699, 'learning_rate': 8.523735002608243e-06, 'epoch': 0.57}\n",
      "{'loss': 10.9699, 'learning_rate': 7.480438184663538e-06, 'epoch': 0.63}\n",
      "{'loss': 10.9699, 'learning_rate': 6.437141366718832e-06, 'epoch': 0.68}\n",
      "{'loss': 10.9699, 'learning_rate': 5.3938445487741264e-06, 'epoch': 0.73}\n",
      "{'loss': 10.9699, 'learning_rate': 4.350547730829422e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\Python\\NLP_100\\Chapter10\\Ex92-test.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/Python/NLP_100/Chapter10/Ex92-test.ipynb#W0sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/Python/NLP_100/Chapter10/Ex92-test.ipynb#W0sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/Python/NLP_100/Chapter10/Ex92-test.ipynb#W0sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     model,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/Python/NLP_100/Chapter10/Ex92-test.ipynb#W0sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/Python/NLP_100/Chapter10/Ex92-test.ipynb#W0sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Code/Python/NLP_100/Chapter10/Ex92-test.ipynb#W0sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m )\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Code/Python/NLP_100/Chapter10/Ex92-test.ipynb#W0sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1922\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1920\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 1922\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1923\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1924\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2282\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[0;32m   2281\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> 2282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[0;32m   2283\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2387\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2379\u001b[0m         smp\u001b[39m.\u001b[39msave(\n\u001b[0;32m   2380\u001b[0m             opt_state_dict,\n\u001b[0;32m   2381\u001b[0m             os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, OPTIMIZER_NAME),\n\u001b[0;32m   2382\u001b[0m             partial\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2383\u001b[0m             v3\u001b[39m=\u001b[39msmp\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mshard_optimizer_state,\n\u001b[0;32m   2384\u001b[0m         )\n\u001b[0;32m   2385\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_deepspeed_enabled \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfsdp \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fsdp_enabled):\n\u001b[0;32m   2386\u001b[0m     \u001b[39m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[1;32m-> 2387\u001b[0m     torch\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstate_dict(), os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_dir, OPTIMIZER_NAME))\n\u001b[0;32m   2389\u001b[0m \u001b[39m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[0;32m   2390\u001b[0m is_deepspeed_custom_scheduler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_deepspeed_enabled \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   2391\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[0;32m   2392\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:850\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[39m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[39m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[39m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39mif\u001b[39;00m storage\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 850\u001b[0m     storage \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39;49mcpu()\n\u001b[0;32m    851\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    852\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\storage.py:134\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mUntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39;49mcopy_(\u001b[39mself\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from os import linesep, read\n",
    "from datasets import Dataset, load_metric\n",
    "import datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" \n",
    "device = torch.device('cuda')\n",
    "# print(device)\n",
    "# print(torch.cuda.get_device_name(device))\n",
    "\n",
    "\n",
    "def create_dataset(englishScript, germanScript):\n",
    "    lines_en = []\n",
    "    lines_de = []\n",
    "    with open(englishScript, mode='r', encoding='utf-8') as file_in:\n",
    "        for line in file_in:\n",
    "            lines_en.append(line.strip())\n",
    "\n",
    "    with open(germanScript, mode='r', encoding='utf-8') as file_in:\n",
    "        for line in file_in:\n",
    "            lines_de.append(line.strip())\n",
    "\n",
    "    translation_dict = {\"translation\": []}\n",
    "\n",
    "    for i in range(len(lines_en)):\n",
    "        singledict = {\n",
    "            \"en\": lines_en[i], \"de\": lines_de[i]}\n",
    "        translation_dict[\"translation\"].append(singledict)\n",
    "    return translation_dict\n",
    "\n",
    "\n",
    "translation_dict_train = create_dataset(\n",
    "    \"./prep/train.de-en.en\", \"./prep/train.de-en.de\",)\n",
    "translation_dict_test = create_dataset(\n",
    "    \"./prep/test.de-en.en\", \"./prep/test.de-en.de\")\n",
    "translation_dict_valid = create_dataset(\n",
    "    \"./prep/valid.de-en.de\", \"./prep/valid.de-en.de\")\n",
    "\n",
    "\n",
    "# books = load_dataset(\"opus_books\", \"en-fr\")\n",
    "# dataset = load_dataset(\"text\", data_files={\n",
    "#                        \"train\": \"./prep/train.de-en.en\", \"validation\": \"./prep/train.de-en.de\"})\n",
    "\n",
    "dataset_train = Dataset.from_dict(translation_dict_train)\n",
    "dataset_test = Dataset.from_dict(translation_dict_test)\n",
    "dataset_valid = Dataset.from_dict(translation_dict_valid)\n",
    "\n",
    "dataset_final = DatasetDict(\n",
    "    {\"train\": dataset_train, \"test\": dataset_test, \"validation\": dataset_valid})\n",
    "\n",
    "print(dataset_final)\n",
    "\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"de-DE\"\n",
    "\n",
    "prefix = \"\"\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to German: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "source_lang = \"en\"\n",
    "target_lang = \"de\"\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset_final.map(preprocess_function, batched=True)\n",
    "# train_dataloader = DataLoader(\n",
    "#     dataset=dataset_final, batch_size=4, shuffle=True, collate_fn=lambda x: x)\n",
    "# tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "#     dataset_final[\"train\"].column_names)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(\n",
    "        decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(\n",
    "        pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
